<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/moon.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
        <section>
          <h3> Audio waveform synthesis using deep learning </h3>
          <ul>
            <li> motivation </li>
            <li> introduction to sound and acoustics, </li>
            <li> some challenges with audio synthesis, </li>
            <li> generative methods, and </li>
            <li> take a look at a recently proposed adversarial methods. </li>
          </ul>
        </section>

        <section>
          <h3> Overview of audio synthesis using deep learning </h3>
          <ul>
            <li> Reference: "Generating music in the waveform domain", by Sander Dieleman </li>
            <li> https://benanne.github.io/2020/03/24/audio-generation.html </li>
          </ul>
        </section>

        <section>
          <h3> Why audio?</h3>
          <img src="https://benanne.github.io/images/player_piano.jpg">
          <aside class="notes">
            Some traditional music synthesis models take a symbolic
            representation of the audio as input and transforms this into sound.
            Here, we are looking at a fairly old music synthesis model of a
            piano, which takes a piano-roll as input and then the mechanical
            machinery inside the piano translates the piano-roll into
            pressing the correct piano keys. 
          </aside>
        </section>

        <section>
          <h3> Why audio? </h3>
          <img src="https://benanne.github.io/images/piano_roll.jpg">
          <ul>
            <li> captures many degrees of freedom for a piano </li>
            <li> what if we do not have the symbolic to audio model? </li>
          </ul>
          <aside class="notes">
            With digitalization we have taken the step from such mechanical models towards
            digital models that can translate the symbolic input into music or
            audio.
            Early research on music generation has focused on modelling such
            symbolic representations of the audio and then use pre-defined
            models to transform the generated symbolic representation into
            sound. This is a very reasonable first step, but in other domains
            where we may not have access to models that
            translates symbolic representations into audio, for example in
            speech synthesis, we need to think about how we can create such
            models. A research direction that has shown some promise the
            last years is to learn these models from data.
          </aside>
        </section>

        <section>
          <h3> Frequency representation </h3>
          <ul>
            <li> for human and computational analysis we often use <b>spectrograms</b> </li>
            <li> shows the <b> local frequency content of the signal over time </b></li>
          </ul>
        </section>

        <section>
          <h3> Frequency representation </h3>
          <p> Spectrograms represent both amplitude and the phase </p>
          <img src="https://benanne.github.io/images/spectrogram_magnitude.png" class="fragment">
          <img src="https://benanne.github.io/images/spectrogram_phase.png" class="fragment">
          <p class ="fragment" data-audio-src="https://benanne.github.io/files/original_phase.wav"> </p>
          <p class ="fragment" data-audio-src="https://benanne.github.io/files/random_phase.wav"> </p>
          <aside class="notes">
            Spectrograms are complex-valued and represent both the amplitude and
            the phase of the local frequency content of the signal over time.
            Let's look at a visualization of the amplitude component. It
            contains a lot of structure, and has been widely used for prediction
            tasks on audio. I used the amplitude spectrogram during my master's
            thesis to predict the species of a bird in a given sound recording.
            What has largely been ignored is the phase component, which looks a
            lot more like noise, and which has proven hard to model. When
            extracting information from audio signals it turns out that we can
            often just discart the phase component, but when we want to generate
            sound it is very important because it affects our perception. Lets
            listen to two piano pieces to make this clear. The first one I will
            play is the original sound with the original amplitude and
            phase component, and in the second example we will replace the phase
            component with a random phase component. Harmony is preserved, but
            the timbre changes completely. Note also that most of us would still
            probably be able to classify the second sound as a piano, but we
            would all agree that it sounds off.
          </aside>
        </section>
        <section>
          <h3> What is hard about the phase? </h3>
          <ul>
            <li> it is an <b>angle</b>: $\phi \in [0,2\pi)$ and it wraps around, </li>
            <li> <b>effectively random</b> as amplitude tends to 0, </li>
            <li> absolute phase less meaningful, </li>
            <li> relative phase matter perceptually, </li>
          </ul>
          <aside class="notes">
          </aside>
        </section>

        <section>
          <h3> Why waveforms? </h3>
          <ul>
            <li> by modelling the waveform we are implicitly modelling the phase </li>
            <li> waveform modelling most common in the generative setting </li>
            <li> magnitude spectrograms most common in the discriminative setting </li>
          </ul>
          <aside class="notes">
          </aside>
        </section>

        <section>
          <h3> Challenges for machine learning to model waveforms? </h3>
          <ul>
            <li> high dimensional data </li>
            <li> long ranging dependencies </li>
            <li> cyclic dependencies </li>
          </ul>
          <aside class="notes">
            If we only consider few second segments of audio at a 44kHz sample
            rate the resolution is comparable to a high resolution image, and we
            are making substantial progress on high resolution images, so I
            think that this argument is a bit overplayed. State-of-the-art image
            generation models can generate images with up to a million
            dimensions, 1024x1024 pixels, which would correspond to sound
            segments of 22 seconds at 44kHz, and we are struggling with segments
            of a couple of seconds at 16KHz. It is still very high dimensional
            data, but possibly not the whole story of why it is challenging. A
            clear difference is that we have very long ranging dependencies in
            audio, which is not as much of a problem in the image domain, 
          </aside>
        </section>

        <section>
          <h3> Generative models </h3>
          <ul>
            <li> Dataset $X$ of examples $x \in \mathcal{X}$ </li>
            <li> $x \sim p(x)$, assumed i.i.d </li>
            <li> learn generative model $p(x|\theta)$ that approximates $p(x)$ </li>
          </ul>
        </section>

        <section>
          <h3> Conditional generative models </h3>
          <ul>
            <li> Dataset $X,Y$ of examples $x \in \mathcal{X}$ and $y \in
              \mathcal{Y}$</li>
            <li> $x,y\sim p(x,y)$ </li>
            <li> learn generative model $p(x|y; \theta)$ that approximates $p(x|y)$ </li>
            <li> <i>explicit</i>: allows $p(x|y; \theta)$ and $x \sim p(x|y; \theta)$ </li>
            <li> <i>implicit</i>: allows only $x \sim p(x|y; \theta)$ </li>
          </ul>
        </section>

        <section>
          <h3> Different model settings </h3>
          <ul>
            <li> likelihood-based models </li>
            <ul>
              <li> autoregressive models </li>
              <li> flow-based models </li>
              <li> variational autoencoders</li>
            </ul>
            <li> <b>adversarial models</b> </li>
          </ul>
          <aside class="notes">
            likelihood-based models are <i> explicit </i> models, meaning that
            we can infer the likelihood of a given data point, and sample from
            them. adversarial models are <i> implicit </i> models, meaning that
            we can not infer the likelihood of a given data point, but we can
            sample from them.
          </aside>
        </section>

        <section>
          <h3> Mode-covering vs. mode-seeking </h3>
          <img src="https://benanne.github.io/images/mode_seeking_covering.png"> 
          <aside class="notes">
            roughly speaking we can make another distinction between these two
            types of models. likelihood-based models tend to be mode-covering,
            while adversarial models tend to be mode-seeking.
          </aside>
        </section>

        <section>
          <h3> Problem categorization </h3>
          <img src="https://benanne.github.io/images/sparse-dense-conditioning.svg">
          <aside class="notes">
            I like the categorization that Dielman does in his blog-post of
            different types of conditioning inputs. He defines a sparse
            condition as a condition that reduces the uncertainty of the correct
            output very little, and a dense condition as a condition that
            reduces the uncertainty of the correct output a lot. Basically, we
            say that we have a sparse condtiion if the input contains little
            information about the correct output, and we have a dense condition
            if the input contains a lot of information about the output.
          </aside>
        </section>

        <section>
          <h3> Problem categorization </h3>
          <ul>
            <li> densely conditioned and sparsely generated output: e.g.,
              waveform to piano-roll (low variability) </li>
            <li> sparse condition and sparse-ish output: e.g., noise to piano-roll, MIDI</li>
            <li> dense-ish condition and dense output: e.g., text or spectrogram to waveform </li>
            <li> sparse condition and dense output: e.g., noise to waveform
              (high variability) </li>
          </ul>
          <aside class="notes">
            Progress has been made on sparsely conditioned models with sparsely
            generated output, for example noise and the music category we want
            to model and output a symbolic representation of that music category.
            (TODO: examples?), we have also made progress on 
            models, both learned from data, and defined by some well known model
            of that sound class, but we have only just begun to make progress on
            something-to-sound models, that take as input, for example a sound
            class, and outputs sound. This is all very vague, so lets make this
            a bit more precise.

            A lot of progress has been made on
            sparse condition and sparse output models, and dense condition and
            dense output models, and we are starting to see progress on sparse
            condition and dense output models in audio synthesis.
          </aside>
        </section>

        <section>
          <h3> Sparse condition and sparse-ish output </h3>
          <img src="images/noise2midi.png" >
          <p class ="fragment" data-audio-src="http://mogren.one/files/c-rnn-gan-sample11.mp3"> </p>
          <p class ="fragment" data-audio-src="http://mogren.one/files/c-rnn-gan-sample.mp3"> </p>
        </section>

        <section>
          <h3> Sparse to denser to dense via symbolic representation </h3>
          <img src="images/wave2midi2wave.png" >
          <p class ="fragment" data-audio-src=""> </p>
          <p class ="fragment" data-audio-src=""> </p>
        </section>

        <section>
          <h3> Sparse condition and dense output </h3>
          <img src="images/noise2wave.png" >
          <p class ="fragment" data-audio-src=""> </p>
          <p class ="fragment" data-audio-src=""> </p>
        </section>

        <section>
          <h3> WaveGAN and SpecGAN </h3>
          <ul>
            <li> TODO: talk about WaveGAN and SpecGAN </li>
            <li> listen to examples </li>
          </ul>
        </section>

        <section>
          <h3> MelGAN</h3>
          <ul>
            <li> TODO: talk about MelGAN </li>
            <li> listen to examples </li>
          </ul>
        </section>

        <section>
          <h3> SpecGAN + MelGAN </h3>
          <ul>
            <li> TODO: talk about combining SpecGAN and MelGAN </li>
            <li> listen to examples </li>
          </ul>
        </section>

			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
		Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				mouseWheel: true,
				audio: {
					prefix: 'audio-slideshow/',
					suffix: '.ogg',
					defaultDuration: 5,
					textToSpeechURL: "http://api.voicerss.org/?key=[YOUR_KEY]&hl=en-gb&c=ogg&src=",
					advance: 500,
					autoplay: false,
					defaultNotes: true,
					defaultText: false,
					playerOpacity: 0.2,
				},
				menu: { // Menu works best with font-awesome installed: sudo apt-get install fonts-font-awesome
					themes: false,
					transitions: false,
					markers: true,
					hideMissingTitles: true,
					custom: [
				            { title: 'Plugins', icon: '<i class="fa fa-external-link-alt"></i>', src: 'toc.html' },
				            { title: 'About', icon: '<i class="fa fa-info"></i>', src: 'about.html' }
				        ]
				},
				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'none', // none/fade/slide/convex/concave/zoom

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/audio-slideshow/RecordRTC.js', condition: function( ) { return !!document.body.classList; } },				
					{ src: 'plugin/audio-slideshow/slideshow-recorder.js', condition: function( ) { return !!document.body.classList; } },				
					{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } },
					{ src: 'plugin/menu/menu.js' },
					{ src: 'plugin/notes/notes.js' },
					{ src: 'plugin/math/math.js' },
				],
				keyboard: { 
					82: function() { Recorder.toggleRecording(); },	// press 'r' to start/stop recording
					90: function() { Recorder.downloadZip(); }, 	// press 'z' to download zip containing audio files
					84: function() { Recorder.fetchTTS(); } 	// press 't' to fetch TTS audio files
  				}
			});
		</script>
	</body>
</html>
