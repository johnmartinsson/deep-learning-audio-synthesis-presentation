<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/moon.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
        <section>
          <h3> Audio waveform synthesis using deep learning </h3>
          <ul>
            <li> motivation </li>
            <li> introduction to sound and acoustics, </li>
            <li> some challenges with audio synthesis, </li>
            <li> generative methods, and </li>
            <li> take a look at a recently proposed adversarial methods. </li>
          </ul>
          <p class ="fragment" data-audio-src="audio/real_piano.wav"> test audio </p>
        </section>

        <section>
          <h3> Overview of audio synthesis using deep learning </h3>
          <ul>
            <li> Reference: "Generating music in the waveform domain", by Sander Dieleman </li>
            <li> https://benanne.github.io/2020/03/24/audio-generation.html </li>
          </ul>
        </section>

        <section>
          <h3> Why audio?</h3>
          <img src="https://benanne.github.io/images/player_piano.jpg">
          <aside class="notes">
            Some traditional music synthesis models take a symbolic
            representation of the audio as input and transforms this into sound.
            Here, we are looking at a fairly old music synthesis model of a
            piano, which takes a piano-roll as input and then the mechanical
            machinery inside the piano translates the piano-roll into
            pressing the correct piano keys. 
          </aside>
        </section>

        <section>
          <h3> Why audio? </h3>
          <img src="https://benanne.github.io/images/piano_roll.jpg">
          <!--
          <ul>
            <li> captures many degrees of freedom for a piano </li>
            <li> what if we do not have the symbolic to audio model? </li>
          </ul>
          -->
          <aside class="notes">
            With digitalization we have taken the step from such mechanical models towards
            digital models that can translate the symbolic input into music or
            audio.
            Early research on music generation has focused on modelling such
            symbolic representations of the audio and then use pre-defined
            models to transform the generated symbolic representation into
            sound. This is a very reasonable first step, but in other domains
            where we may not have access to models that
            translates symbolic representations into audio, for example in
            speech synthesis, we need to think about how we can create such
            models. A research direction that has shown some promise the
            last years is to learn these models from data.
          </aside>
        </section>

        <section>
          <h3> Frequency representation </h3>
          <ul>
            <li> for human and computational analysis we often use <b>spectrograms</b> </li>
            <li> shows the <b> local frequency content of the signal over time </b></li>
          </ul>
        </section>

        <section>
          <h3> Frequency representation </h3>
          <p> Spectrograms represent both amplitude and the phase </p>
          <img src="https://benanne.github.io/images/spectrogram_magnitude.png" class="fragment">
          <img src="https://benanne.github.io/images/spectrogram_phase.png" class="fragment">
          <p class ="fragment" data-audio-src="https://benanne.github.io/files/original_phase.wav"> </p>
          <p class ="fragment" data-audio-src="https://benanne.github.io/files/random_phase.wav"> </p>
          <aside class="notes">
            Spectrograms are complex-valued and represent both the amplitude and
            the phase of the local frequency content of the signal over time.
            Let's look at a visualization of the amplitude component. It
            contains a lot of structure, and has been widely used for prediction
            tasks on audio. I used the amplitude spectrogram during my master's
            thesis to predict the species of a bird in a given sound recording.
            What has largely been ignored is the phase component, which looks a
            lot more like noise, and which has proven hard to model. When
            extracting information from audio signals it turns out that we can
            often just discart the phase component, but when we want to generate
            sound it is very important because it affects our perception. Lets
            listen to two piano pieces to make this clear. The first one I will
            play is the original sound with the original amplitude and
            phase component, and in the second example we will replace the phase
            component with a random phase component. Harmony is preserved, but
            the timbre changes completely. Note also that most of us would still
            probably be able to classify the second sound as a piano, but we
            would all agree that it sounds off.
          </aside>
        </section>
        <section>
          <h3> What is hard about the phase? </h3>
          <ul>
            <li> it is an <b>angle</b>: $\phi \in [0,2\pi)$ and it wraps around, </li>
            <li> <b>effectively random</b> as amplitude tends to 0, </li>
            <li> absolute phase less meaningful, </li>
            <li> relative phase matter perceptually, </li>
          </ul>
          <aside class="notes">
          </aside>
        </section>

        <section>
          <h3> Why waveforms? </h3>
          <ul>
            <li> by modelling the waveform we are implicitly modelling the phase </li>
            <li> waveform modelling common in the generative setting </li>
            <li> magnitude spectrograms common in the discriminative setting </li>
          </ul>
          <aside class="notes">
          </aside>
        </section>

        <section>
          <h3> Challenges for machine learning to model waveforms? </h3>
          <ul>
            <li> high dimensional data </li>
            <li> cyclic dependencies common </li>
            <li> long ranging dependencies </li>
          </ul>
          <aside class="notes">
            If we only consider few second segments of audio at a 44kHz sample
            rate the resolution is comparable to a high resolution image, and we
            are making substantial progress on high resolution images, so I
            think that this argument is a bit overplayed. State-of-the-art image
            generation models can generate images with up to a million
            dimensions, 1024x1024 pixels, which would correspond to sound
            segments of 22 seconds at 44kHz, and we are struggling with segments
            of a couple of seconds at 16KHz. It is still very high dimensional
            data, but possibly not the whole story of why it is challenging. A
            clear difference is that we have very long ranging dependencies in
            audio, which is not as much of a problem in the image domain, 
          </aside>
        </section>

        <section>
          <h3> Generative models </h3>
          <ul>
            <li> Dataset $X$ of examples $x \in \mathcal{X}$ </li>
            <li> $x \sim p(x)$, assumed i.i.d </li>
            <li> learn generative model $p(x|\theta)$ that approximates $p(x)$ </li>
          </ul>
        </section>

        <section>
          <h3> Conditional generative models </h3>
          <ul>
            <li> Dataset $X,Y$ of examples $x \in \mathcal{X}$ and $y \in
              \mathcal{Y}$</li>
            <li> $x,y\sim p(x,y)$ </li>
            <li> learn generative model $p(x|y; \theta)$ that approximates $p(x|y)$ </li>
            <li> <i>explicit</i>: allows $p(x|y; \theta)$ and $x \sim p(x|y; \theta)$ </li>
            <li> <i>implicit</i>: allows only $x \sim p(x|y; \theta)$ </li>
          </ul>
        </section>

        <section>
          <h3> Different model settings </h3>
          <ul>
            <li> likelihood-based models </li>
            <ul>
              <li> autoregressive models: e.g., WaveNet </li>
              <li> flow-based models: e.g., WaveGlow </li>
              <!-- <li> variational autoencoders: not popular</li> -->
            </ul>
            <li> <b>adversarial models</b> </li>
          </ul>
          <aside class="notes">
            likelihood-based models are <i> explicit </i> models, meaning that
            we can infer the likelihood of a given data point, and sample from
            them. adversarial models are <i> implicit </i> models, meaning that
            we can not infer the likelihood of a given data point, but we can
            sample from them.
          </aside>
        </section>

        <section>
          <h3> Adversarial model </h3>
          <ul>
            <li> $\mathcal{L} = \mathbb{E}_{x} [log D(x)] + \mathbb{E}_{z}[log(1-D(G(z)))]$</li>
            <li class="fragment"> discriminator trained to maximizes this loss </li>
            <li class="fragment"> generator trained to minimize it</li>
            <li class="fragment"> pro: realistic samples </li>
            <li class="fragment"> con: usually less diverse </li>
          </ul>
        </section>

        <section>
          <h3> Mode-covering vs. mode-seeking </h3>
          <img src="https://benanne.github.io/images/mode_seeking_covering.png"> 
          <aside class="notes">
            roughly speaking we can make another distinction between these two
            types of models. likelihood-based models tend to be mode-covering,
            while adversarial models tend to be mode-seeking.
          </aside>
        </section>

        <section>
          <h3> Problem categorization </h3>
          <img src="https://benanne.github.io/images/sparse-dense-conditioning.svg">
          <aside class="notes">
            I like the categorization that Dielman does in his blog-post of
            different types of conditioning inputs. He defines a sparse
            condition as a condition that reduces the uncertainty of the correct
            output very little, and a dense condition as a condition that
            reduces the uncertainty of the correct output a lot. Basically, we
            say that we have a sparse condtiion if the input contains little
            information about the correct output, and we have a dense condition
            if the input contains a lot of information about the output.
          </aside>
        </section>

        <section>
          <h3> Problem categorization </h3>
          <ul>
            <li> densely conditioned and sparsely generated output: e.g., waveform to piano-roll (low variability) </li>
            <li> sparse condition and sparse-ish output: e.g., noise to piano-roll, MIDI</li>
            <li> dense-ish condition and dense output: e.g., text or spectrogram to waveform </li>
            <li> sparse condition and dense output: e.g., noise to waveform (high variability) </li>
          </ul>
          <aside class="notes">
            Progress has been made on sparsely conditioned learned models with sparsely
            generated output, for example mapping noise to a symbolic
            representation of a certain music category.
            We have also made progress on 
            models, both learned from data, and manually defined models from
            symbolic representations, such as MIDI to piano sound, but I think
            that we have only just begun to see progress on
            models that take as condition for example a sound
            class, and outputs naural and realistic sound.

            A lot of progress has been made on
            sparse condition and sparse output models, and dense condition and
            dense output models, and we are starting to see progress on sparse
            condition and dense output models in audio synthesis. In other
            words, early work has tried to train generative models for e.g.,
            MIDI representations and then sample from such a model and use a
            pre-defined MIDI to sound model to finally generate a sound wave, we
            have also seen works on learning MIDI to sound models, but we are
            still in the early stages of generative models that are trained on
            only sound data without intermediate representations such as MIDI,
            or in the speech domain, text.
          </aside>
        </section>

        <section>
          <h3> Sparse condition and sparse-ish output </h3>
          <img src="images/noise2midi.png" >
          <p> Mogren, 2016, NeurIPS workshop </p>
          <p class ="fragment" data-audio-src="https://mogren.one/files/c-rnn-gan-sample11.mp3"> Sample 1</p>
          <p class ="fragment" data-audio-src="https://mogren.one/files/c-rnn-gan-sample.mp3"> Sample 2</p>
          <aside class="notes">
            <ul>
              <li> Olof Mogrens work from 2016. </li>
              <li> noise to MIDI generative model </li>
              <li> C-RNN-GAN </li>
            </ul>
          </aside>
        </section>

        <section>
          <h3> Wave2MIDI2Wave </h3>
          <img src="images/wave2midi2wave.png" >
          <p> Hawthorne et. al, 2019, ICLR </p>
          <p class ="fragment" data-audio-src="audio/sample_0.mp3"> defined MIDI to audio</p>
          <p class ="fragment" data-audio-src="audio/sample_0_wn.mp3"> learned MIDI to audio</p>
        </section>

        <section>
          <h3> Sparse condition and dense output </h3>
          <img src="images/noise2wave.png" >
          <p> Donahue et. al, 2019, ICLR </p>
          <p class ="fragment" data-audio-src="audio/wavegan_piano.wav"> WaveGAN piano</p>
        </section>

        <section>
          <h3> Adversarial audio synthesis (Donahue et. al, 2019)</h3>
          <ul>
            <li> more likely to exhibit periodicity </li>
            <li> correlations across large windows common </li>
            <li> need larger receptive fields </li>
            <li> alteration of DCGAN </li>
          </ul>
        </section>

        <section>
          <h3> DCGAN (Radford et. al, 2015)</h3>
          <img src="https://gluon.mxnet.io/_images/dcgan.png" >
        </section>

        <section>
          <h3> WaveGAN (Donahue et. al, 2019)</h3>
          <ul>
            <li> 2D kernels size 5x5 -> 1D kernels size 25</li>
            <li> transposed stride 2 -> 4 </li>
            <li> remove batch norm (missing ablation) </li>
            <li> train using WGAN-GP (Gulrajani et al., 2017) strategy </li>
            <li> add one layer: 4096 -> 16384 output </li>
          </ul>
        </section>

        <section>
          <h3> SpecGAN (Donahue et. al, 2019)</h3>
          <ul>
            <li> basically DCGAN </li>
            <li> trained on amplitude spectrograms </li>
            <li> Griffin-Lim to estimate phase from spectrogram </li>
          </ul>
        </section>

        <section>
          <h3> Adversarial audio synthesis (Donahue et. al, 2019)</h3>
          <img src="images/wavegan_spectrograms.png" >

          <p class ="fragment" data-audio-src="audio/real_piano.wav"> </p>
          <p class ="fragment" data-audio-src="audio/wavegan_piano.wav"> </p>
          <p class ="fragment" data-audio-src="audio/specgan_piano.wav"> </p>
        </section>

        <section>
          <h3> Adversarial audio synthesis (Donahue et. al, 2019)</h3>
          <img src="images/wavegan_table.png" >
        </section>

        <section>
          <h3> MelGAN (Kumar et. al, 2019)</h3>
          <ul>
            <li> learned model for Mel spectrogram inversion </li>
          </ul>
          <p class ="fragment" data-audio-src="audio/original.wav"> </p>
          <p class ="fragment" data-audio-src="audio/melgan.wav"> </p>
          <p class ="fragment" data-audio-src="audio/griffin_lim.wav"> </p>
        </section>

        <section>
          <h3> MelGAN: Generator </h3>
          <ul>
            <li> condition: mel-spectrogram </li>
            <li> output: waveform </li>
            <li> residual blocks with dilated convolutions </li>
            <li> stride and kernel-size carefully chosen to mitigate "checkerboard patterns" </li>
            <li> normaliation technique important: use weight normalization </li>
          </ul>
          <aside class="note">

          </aside>
        </section>

        <section>
          <h3> MelGAN: Discriminator </h3>
          <ul>
            <li> multi-scale architecture </li>
            <li> $D_1$, $D_2$, $D_3$: raw audio donwsample 1, 2, and 4 respectively</li>
            <li> intuition: each discriminator biased towards different frequecies </li>
            <li> window-based objective </li>
          </ul>
          <aside class="note">

          </aside>
        </section>

        <section>
          <h3> SpecGAN + MelGAN </h3>
          <ul>
            <li> SpecGAN: noise to spectrogram </li>
            <li> MelGAN: spectrogram to waveform</li>
          </ul>
          <p class ="fragment" data-audio-src="audio/specgan_sc09.wav"> </p>
          <p class ="fragment" data-audio-src="audio/specgan_melgan_sc09.wav"> </p>
        </section>

			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
		Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				mouseWheel: true,
				audio: {
					prefix: 'audio-slideshow/',
					suffix: '.ogg',
					defaultDuration: 5,
					textToSpeechURL: "http://api.voicerss.org/?key=[YOUR_KEY]&hl=en-gb&c=ogg&src=",
					advance: 500,
					autoplay: false,
					defaultNotes: true,
					defaultText: false,
					playerOpacity: 0.2,
				},
				menu: { // Menu works best with font-awesome installed: sudo apt-get install fonts-font-awesome
					themes: false,
					transitions: false,
					markers: true,
					hideMissingTitles: true,
					custom: [
				            { title: 'Plugins', icon: '<i class="fa fa-external-link-alt"></i>', src: 'toc.html' },
				            { title: 'About', icon: '<i class="fa fa-info"></i>', src: 'about.html' }
				        ]
				},
				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'none', // none/fade/slide/convex/concave/zoom

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/audio-slideshow/RecordRTC.js', condition: function( ) { return !!document.body.classList; } },				
					{ src: 'plugin/audio-slideshow/slideshow-recorder.js', condition: function( ) { return !!document.body.classList; } },				
					{ src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } },
					{ src: 'plugin/menu/menu.js' },
					{ src: 'plugin/notes/notes.js' },
					{ src: 'plugin/math/math.js' },
				],
				keyboard: { 
					82: function() { Recorder.toggleRecording(); },	// press 'r' to start/stop recording
					90: function() { Recorder.downloadZip(); }, 	// press 'z' to download zip containing audio files
					84: function() { Recorder.fetchTTS(); } 	// press 't' to fetch TTS audio files
  				}
			});
		</script>
	</body>
</html>
